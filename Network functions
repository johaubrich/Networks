#These functions were developed by Justin W. Kenney (https://github.com/jkenney9a) and only slightly adapted to fit my goals

# libraries

library(igraph) #For network functions
library(Hmisc) #For generating correlation/p-value matrices
library(boot) #For bootstrapping
library(data.table) #for rbindlist
library(proxy) #For distance measures (e.g, Jaccard distance)
library(statGraph) 

# Functions

# correlation matrices
corr_matrix <- function(df, p.adjust.method='none', type='pearson'){
  #   Input: Dataframe with headers as titles (brain regions)
  #           Whether or not to apply a p-value adjustment method drawn from the 'p.adjust' function
  #           (e.g, 'fdr', 'bonferroni' etc.). 
  #           whether to use 'pearson' or 'spearman' correlation
  #           NOTE: assume undirected graph!
  #   
  #   Output: List of two dataframes corresponding to 1) all pairwise Pearson 
  #   correlations and 2) all associated un-adjusted p-values
  
  
  corr <- rcorr(as.matrix(df), type=type)
  df_corr <- as.data.frame(corr['r'])
  
  #adjust p-values if necessary
  adjusted.P <- corr[['P']]
  adjusted.P[upper.tri(adjusted.P)] <- p.adjust(adjusted.P[upper.tri(adjusted.P)], method=p.adjust.method)
  adjusted.P[lower.tri(adjusted.P)] <- NA
  df_pvalue <- as.data.frame(adjusted.P)
  
  
  names(df_corr) <- gsub("^r.","",colnames(df_corr), fixed=FALSE) #Remove "r." from node names
  
  
  #Change ... to -; for some reason when R imports "-" it turns it into "..."
  names(df_corr) <- gsub("...","-",colnames(df_corr), fixed=TRUE)
  rownames(df_corr) <- gsub("...", "-", rownames(df_corr), fixed=TRUE)
  names(df_corr) <- gsub("sums.", "", colnames(df_corr), fixed=TRUE)
  rownames(df_corr) <- gsub("sums.", "", rownames(df_corr), fixed=TRUE)
  
  names(df_corr) <- rownames(df_corr)
  
  return(list("corr" = df_corr,"pvalue" = df_pvalue))
}


## Treshold
corr_matrix_threshold <- function(df, negs = FALSE, thresh=0.01, thresh.param='p', p.adjust.method='bonferroni',
                                  type='pearson'){
  #   Input: Dataframe with headers as titles (brain regions) of counts etc.
  #           threshold (can be a list of thresholds now) and threshold parameter (p, r, or cost), whether or not to keep negative correlations.
  #           and the p-value adjustment method if using p-value as threshold parameter.
  #           and whether to use pearson or spearman correlation
  #   
  #   Output: Dataframe of correlations thresholded at p < threshold
  #   
  #   NOTE: Removes diagonals
  
  dfs <- corr_matrix(df, p.adjust.method=p.adjust.method, type=type)
  df_corr <- dfs[['corr']]
  df_pvalue <- dfs[['pvalue']]
  
  #remove diagonals (may not be necessary when using igraph...)
  diag(df_corr) <- 0
  diag(df_pvalue) <- NA
  #df_corr[mapply("==", df_corr, 1)] <- 0
  
  #remove any NaNs, infs or NAs (sometimes happens with bootstrapping; not sure why)
  df_pvalue[mapply(is.infinite, df_corr)] <- 1
  df_pvalue[mapply(is.nan, df_corr)] <- 1  
  df_pvalue[mapply(is.na, df_pvalue)] <- 1
  df_corr[mapply(is.infinite, df_corr)] <- 0
  df_corr[mapply(is.nan, df_corr)] <- 0
  df_corr[mapply(is.na, df_corr)] <- 0
  
  #remove negative correlations
  if(negs == FALSE){
    df_corr[mapply("<", df_corr, 0)] <- 0
  }
  
  if(tolower(thresh.param)=='p'){
    #apply p-value threshold to correlation matrix
    df_corr <- lapply(thresh, function(x) {df_corr[mapply(">=", df_pvalue, x)] <- 0; df_corr})    
  } else if(tolower(thresh.param)=='r'){
    df_corr <- lapply(thresh, function(x) {df_corr[mapply("<=", abs(df_corr), x)] <- 0; df_corr})
  } else if(tolower(thresh.param)=='cost'){
    r.threshold <- quantile(abs(df_corr), probs=1-thresh, na.rm=TRUE)
    df_corr <- lapply(r.threshold, function(x) {df_corr[mapply("<=", abs(df_corr), x)] <- 0; df_corr})
  } else{
    stop("Invalid thresholding parameter")
  }
  
  if(length(thresh) == 1){
    df_corr <- df_corr[[1]]
  }
  
  return(df_corr)
}


# Centrality measures
get_centrality_measures <-function(G, weighted=FALSE, nodal_efficiency=FALSE, normalized=FALSE, min_max_normalization=FALSE){
  # Input: An igraph graph
  #
  # Output: A dataframe of centrality measures:
  # Degree, betweenness, eigenvector, closeness, 
  # transitivity (~clustering coefficient), nodal efficiency
  # NOTE: nodal efficiency makes use of weights if they exist
  # but has been removed for now b/c of how long the calculation can take!
  if(ecount(G) == 0){
    zeros <- rep(0, vcount(G))
    return(data.frame("degree"=zeros, "betweenness"=zeros, 
                      "eigenvector"=zeros, "closeness"=zeros,
                      "transitivity"=zeros,
                      row.names=V(G)$name))
  }
  
  degree <- igraph::degree(G, normalized=normalized)
  G.pos <- G
  E(G.pos)$weight_abs <- abs(E(G.pos)) #Positive numbers are necessary for betweenness and closeness
  E(G.pos)$weight <- 1/abs(E(G.pos)$weight_abs) #bet and clo are based on distance graphs. Here, values are inverted so that weaker links have higher lengths
  
  
  if(weighted==FALSE){
    eigenvector <- evcent(G, weights=NA)
    between <- betweenness(G.pos, normalized=normalized, weights=NA)
    close <- closeness(G.pos, weights=NA, normalized=normalized)
  } else{
    eigenvector <- evcent(G)
    between <- betweenness(G.pos, normalized=normalized, weights=NULL)
    close <- closeness(G.pos, normalized=normalized)
  }
  
  trans <- transitivity(G,type="local", isolates='zero')
  
  
  #Need to pull out matrices for efficiency calculations
  #adj_mat <- as.matrix(get.adjacency(G))
  #weight_mat <- as.matrix(get.adjacency(G, attr='weight'))
  #efficiency <- Global_efficiency(G)
  
  output <- data.frame("degree" = degree, "betweenness" = between, 
                       "eigenvector" = eigenvector[[1]], "closeness" = close,
                       "transitivity" = trans)
  
  if(nodal_efficiency==TRUE){
    node_efficiency <- Nodal_efficiency(G, normalized = normalized)
    output$efficiency <- node_efficiency
  }
  
  if(min_max_normalization==TRUE){
    output <- apply(output, MARGIN=2, function(x) {(x-min(x)) / (max(x) - min(x))})
    output <- as.data.frame(output)
    output$degree.betweenness <- output$degree + output$betweenness
    output$transitivity <- trans #Do not normalize transitivity b/c it is already normalized
  }
  
  
  return(output)
}


Global_efficiency <- function(G, weighted=TRUE){
  #Input: igraph graph
  #
  #Output: efficiency; 
  #NOTE: default uses weighted matrix and assumes 
  #the attribute is called "weight" in the graph.
  #NOTE: The absolute value of edges is used as negative correlations
  # result in odd behaviour. From a theoretical perspective I believe this is 
  # ok b/c a neg. correlation tells us the same thing as a positive correlation
  # with respect to the transfer of information across a network
  #
  #NOTE (16/03/16): Altered calculation for weighted networks. We assume a 
  # weighted correlation network. The way the shortest path-length algorithm works
  # is that lower numbers = shorter distances. However, with correlations the 
  # opposite is true. To correct for this, will take the inverse of the weights. This 
  # works out to be the equivalent of taking the harmonic mean for the path length part
  # of the calculation.
  
  
  if(weighted==TRUE & ecount(G) > 0){
    E(G)$weight <- 1 / abs(E(G)$weight)
    eff <- 1/shortest.paths(G)
  }else{
    eff <- 1/shortest.paths(G, weights=NA)
  }
  
  eff[!is.finite(eff)] <- 0
  gl.eff <- mean(eff[upper.tri(eff)])
  
  
  return(gl.eff)
}

Nodal_efficiency <- function(G, weighted=FALSE, normalized=FALSE) {
  #Input: igraph graph, and whether or not to consider weights in the efficiency calculation
  # whether or not to normalize (where max value = 1)
  
  #Output: data frame of nodal efficiency (i.e, average inverse shortest path length from individual node to all other nodes)
  
  if(weighted == TRUE & ecount(G) > 0){
    E(G)$weight <- 1 / abs(E(G)$weight) #Take inverse of weights b/c dealing with correlations
    eff <- 1 / shortest.paths(G)
  }else{
    eff <- 1/shortest.paths(G, weights=NA)
  }
  
  eff[!is.finite(eff)] <- 0
  out <- colSums(eff) / (vcount(G) - 1)
  
  if(normalized == TRUE){
    out <- out / max(out)
  }
  
  return(out)
}

# Small-world random networks
Watts_Strogatz_model <- function(G, iterations=1000, trans_match_iter=100){
  #Input: A graph to use as basis for generating Watts-Strogatz small 
  # world model. Number of random Watts-Strogatz graphs to calculate and
  #the number of iterations to use for matching transitivity/clustering
  #coefficient
  #
  #Output: List containing three items:
  # 1) Random graph statistics and stdev in a df for transitivity and global efficiency
  # 2) The average sorted degree distribution for all the WS graphs generated
  # 3) The average sorted clustering distribution for all the WS graphs generated
  # 4) A dataframe containing all degree and transitivity values from every iteration
  #
  #NOTE: wiring probability for graph is set such that the clustering 
  #coefficient of the Watts-Strogatz graphs are approximately the same
  #as the input graph.
  
  n <- vcount(G) #number of nodes
  e <- ecount(G) #number of edges
  G.trans <- transitivity(G, type="global")
  
  nei <- round(e/n) #Calculate number of edges in each node neighborhood
  #this measure attempts to get as close to the number of 
  #edges as possible to the original graph, but unlikely to be exact
  
  wiring.ps <- seq(0,1,0.01) #generate re-wiring probabilities to test
  
  #Data frame to hold transitivity/clustering values 
  trans.out <- as.data.frame(matrix(NA, ncol=length(wiring.ps), nrow=trans_match_iter, 
                                    dimnames=list(1:trans_match_iter, wiring.ps)))
  
  #Generate a series of Watts-Strogatz graphs to find out what p-value matches the 
  #transitivity/clustering in the original graph
  for(i in 1:trans_match_iter){
    trans.out[i,] <- sapply(wiring.ps, function(x) 
      transitivity(watts.strogatz.game(1,n,nei,x), type="global"))
  }
  
  #Get the p-value associated with the graph matching clustering/transitivity of original graph
  trans.out.means <- colMeans(trans.out)
  p.value <- wiring.ps[which(abs(trans.out.means-G.trans) == min(abs(trans.out.means-G.trans)))]
  
  TR <- c()     #Vector to hold transitivity calculations
  GE <- c()     #Vector to hold global efficiency calculations
  deg.dist <- as.data.frame(matrix(NA, ncol=n, nrow=iterations))
  clust.dist <- as.data.frame(matrix(NA, ncol=n, nrow=iterations))
  
  #Generate many graphs
  for(i in 1:iterations){
    W <- watts.strogatz.game(1,n,nei,p.value)
    TR <- append(TR, transitivity(W, type="global"))
    GE <- append(GE, Global_efficiency(W, weighted=FALSE))
    
    deg.dist[i,] <- sort(igraph::degree(W))
    clust.dist[i,] <- sort(transitivity(W, type="local", isolates='zero'))
  }
  
  df_all_out <- data.frame(degree = unlist(deg.dist), transitivity = unlist(clust.dist))
  
  deg.dist <- colMeans(deg.dist)
  clust.dist <- colMeans(clust.dist)
  
  TR_out <- c(mean(TR), sd(TR))
  GE_out <- c(mean(GE), sd(GE))
  df_out <- data.frame("Transitivity" = TR_out, "Global efficiency" = GE_out, row.names=c("Average", "stdev"))
  
  return (list(df_out, deg.dist, clust.dist, df_all_out))
  
}

# bootstrapping
boot_function <- function(df, indices, thresh=0.01){
  df_boot <- df[indices,]
  df_thresh <- corr_matrix_threshold(df_boot, thresh=thresh, negs=FALSE, thresh.param='p')
  G <- graph.adjacency(as.matrix(df_thresh), mode="undirected",
                       weighted=TRUE)
  
  G <- decompose.graph(G)[[1]]
  GC <- GC_size(G)
  GE <- Global_efficiency(G, weighted=FALSE)
  Trans <- transitivity(G, type="global")
  edges <- length(E(G))
  
  return(c(GC, GE, Trans, edges))
}

bootstrap_measures <- function(df, iterations=500, thresh=0.01, conf_interval=0.95){
  #Input: dataframe of counts (noramlized/cleaned), number of iterations
  #for bootstrapping and p-value for tresholding network, and confidence interval
  #
  #Output: Bootstrap object
  
  return(boot(data=df, statistic=boot_function, R=iterations, thresh=thresh))
  
  
}

